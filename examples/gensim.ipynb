{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news data 불러오기\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park123\\AppData\\Local\\Temp/ipykernel_16304/1303616213.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['documents'].str.replace('[^a-zA-Z]', ' ') # 특수문자 제거\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "news_df = pd.DataFrame({'documents' : documents})\n",
    "news_df['clean_doc'] = news_df['documents'].str.replace('[^a-zA-Z]', ' ') # 특수문자 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : ' '.join([w for w in x.split() if len(w) > 3])) # 글자 3개 이하 다 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : x.lower()) # 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased, disagree, st...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4        [well, change, scoring, playoff, pool, unfortu...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein, israeli, journalist, speak...\n",
       "11310                                                   []\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy, scared, hell, came, last, y...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# 단어의 정수 인코딩과 동시에, 각 뉴스에서 던어의 빈도수 표현\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64281"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary) # 총 학습된 단어의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim 통한 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"price\" + 0.017*\"sale\" + 0.016*\"bike\" + 0.014*\"offer\"')\n",
      "(1, '0.020*\"game\" + 0.018*\"team\" + 0.015*\"year\" + 0.014*\"games\"')\n",
      "(2, '0.034*\"space\" + 0.012*\"nasa\" + 0.007*\"data\" + 0.007*\"launch\"')\n",
      "(3, '0.021*\"file\" + 0.019*\"output\" + 0.017*\"entry\" + 0.011*\"program\"')\n",
      "(4, '0.012*\"colorado\" + 0.010*\"scorer\" + 0.010*\"morris\" + 0.009*\"nist\"')\n",
      "(5, '0.008*\"canada\" + 0.007*\"compass\" + 0.007*\"germany\" + 0.006*\"value\"')\n",
      "(6, '0.014*\"widget\" + 0.009*\"book\" + 0.006*\"client\" + 0.006*\"part\"')\n",
      "(7, '0.016*\"water\" + 0.010*\"picture\" + 0.007*\"radar\" + 0.006*\"sleeve\"')\n",
      "(8, '0.021*\"period\" + 0.012*\"power\" + 0.009*\"gordon\" + 0.009*\"pitt\"')\n",
      "(9, '0.038*\"keyboard\" + 0.032*\"printer\" + 0.030*\"mouse\" + 0.015*\"print\"')\n",
      "(10, '0.014*\"drive\" + 0.010*\"card\" + 0.009*\"system\" + 0.009*\"like\"')\n",
      "(11, '0.009*\"armenian\" + 0.008*\"people\" + 0.008*\"israel\" + 0.008*\"armenians\"')\n",
      "(12, '0.016*\"would\" + 0.011*\"people\" + 0.009*\"like\" + 0.009*\"know\"')\n",
      "(13, '0.015*\"navy\" + 0.014*\"cheers\" + 0.013*\"kent\" + 0.008*\"tyre\"')\n",
      "(14, '0.010*\"conductor\" + 0.010*\"echo\" + 0.008*\"aluminum\" + 0.008*\"ctrl\"')\n",
      "(15, '0.012*\"windows\" + 0.010*\"file\" + 0.010*\"available\" + 0.009*\"program\"')\n",
      "(16, '0.012*\"jesus\" + 0.008*\"believe\" + 0.007*\"christian\" + 0.007*\"bible\"')\n",
      "(17, '0.018*\"food\" + 0.010*\"cross\" + 0.010*\"linked\" + 0.008*\"mask\"')\n",
      "(18, '0.015*\"encryption\" + 0.014*\"chip\" + 0.013*\"keys\" + 0.012*\"clipper\"')\n",
      "(19, '0.013*\"president\" + 0.010*\"national\" + 0.010*\"health\" + 0.008*\"state\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 20\n",
    "# passes = 훈련하는 동안 말뭉치 들어가는 개수\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4, )\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4bf65f30728a6096a7ad3883846392423ebe07b00f52cc917a4c2f9022e0877"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('youtube_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
